Topograph:
  attention_net_architecture:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: &id002
    - 32
    - 32
  edge_net_architecture:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: &id001
    - 256
    - 256
    - 64
  full_connections_jets: false
  full_connections_tops: false
  full_connections_w_top: false
  full_connections_ws: false
  n_iterations: 4
  node_net_architecture:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: *id001
  pooling: att
  top_top_interaction: false
  w_w_interaction: false
all: false
attention_units:
  units: *id002
batch_size: 256
classification_loss:
  weighted: true
config_file: !!python/object/apply:pathlib.PosixPath
- configs
- config_full.yaml
edge_classification:
  activation: gelu
  batch_norm: false
  dropout: 0
  layer_norm: true
  regularization: 0
  units:
  - 128
  - 128
  - 128
graph_dense_net_units:
  units: *id001
initial_graph_block:
  attention_network:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: *id002
  dense_config_edges:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: *id001
  dense_config_nodes:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: *id001
  k_neighbours: 15
  n_iterations: 2
  pooling_edges: att
initialization_top:
  attention_net_architecture:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: *id002
  jets_pooling: att
  regression_net:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    out: 3
    regularization: 0
    units: &id003
    - 64
    - 64
initialization_w:
  first_attention_architecture:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: *id002
  first_w_initialization: att
  regression_net:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    out: 3
    regularization: 0
    units: *id003
  second_attention_architecture:
    activation: gelu
    batch_norm: false
    dropout: 0
    layer_norm: true
    regularization: 0
    units: *id002
  second_w_initialization: att
jet_resolution: false
jet_scale: false
log_dir: !!python/object/apply:pathlib.PosixPath
- log_dir
lr_schedule:
  config:
    first_decay_steps: 2
    initial_learning_rate: 0.001
    m_mul: 1
    t_mul: 1
  name: cosine
matchability: true
n_epochs: 100
n_events: null
network_without_units:
  activation: gelu
  batch_norm: false
  dropout: 0
  layer_norm: true
  regularization: 0
persistent_edges: true
predict: false
regression_loss: mae
regression_net:
  activation: gelu
  batch_norm: false
  dropout: 0
  layer_norm: true
  out: 3
  regularization: 0
  units: *id003
regression_units:
  units: *id003
same_scaler_everything: false
test: false
test_file: ttbar_test.h5
train_file: ttbar_train.h5
use_flavour_tagging: true
val_file: ttbar_val.h5
verbose: 2
